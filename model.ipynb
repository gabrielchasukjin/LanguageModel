{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b14562-a689-4695-8d6d-c5e75ef765da",
   "metadata": {},
   "source": [
    "# üó£ Language Model N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093329fe-78fa-4585-8488-2bc7a57624b9",
   "metadata": {},
   "source": [
    "It's my desire to explore the unknown and discover meaning in data. With the rise of GPT 3, I wanted to create my own language model on a library of over 70,000 eBooks.\n",
    "\n",
    "Summarized Professor Daniel Jurafsky from Stanford University, language models attempt to capture the likelihood that a given sequence of words occur in a given collection of texts. Such a model, for example, could predict that the following sequence has a much higher probability of appearing in a text:\n",
    "- *all of a sudden I notice three guys standing on the sidewalk*\n",
    "\n",
    "than does this same set of words in a different order:\n",
    "\n",
    "- *on guys all I of notice sidewalk three a sudden standing the*\n",
    "\n",
    "As with all statistical models, the true data generating process is unknown to us, so all we can do is **estimate** the probabilities of sentences. For example, one might estimate the probability of a sentence as simply the product of the empirical probabilities (i.e., the number of times a word is observed in a dataset divided by the number of words in that dataset). \n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile})$$\n",
    "\n",
    "\n",
    "I start off by building the simplest model that assigns probabilities to sentences and sequences of words, the **n-gram**. An n-gram is a sequence n-gram of n words: a 2-gram (called bigram) is a two-word sequence of words like ‚Äúplease turn‚Äù, ‚Äúturn your‚Äù, or ‚Äùyour homework‚Äù, and a 3-gram (a trigram) is a three-word sequence of words like ‚Äúplease turn your‚Äù, or ‚Äúturn your homework‚Äù.\n",
    "\n",
    "I use math to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilites to entire sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf7809-c43d-415d-a07e-91293917c589",
   "metadata": {},
   "source": [
    "## üìö Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c530a3b-a425-4823-9a9e-d4c764a29084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747260f1-7ab8-4513-ad40-74517025f2ab",
   "metadata": {},
   "source": [
    "## ü™ö 1) Preparing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d7278-74c7-4eac-a807-293162085c4c",
   "metadata": {},
   "source": [
    "I'll use the `requests` module to download the \"Plain Text UTF-8\" text of a public domain book from [Project Gutenberg](https://www.gutenberg.org/) and prepare it for analysis in later questions. For instance, the book Beowulf's \"Plain Text UTF-8\" URL is [here](https://www.gutenberg.org/ebooks/16328.txt.utf-8), which can be accessed by clicking the \"Plain Text UTF-8\" link [here](https://www.gutenberg.org/ebooks/16328)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb54366-9d2b-4382-b62e-b337e1ca9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieves contents of eBook through HTTP request\n",
    "def get_book(url):\n",
    "    \n",
    "    # HTTP request to Project Gurenberg\n",
    "    text = requests.get(url).text\n",
    "    \n",
    "    # Locate title of Book\n",
    "    title = re.findall(r'Title: ([A-Za-z ]+)', text)[0].upper()\n",
    "    \n",
    "    # Regex --> Located starting point of given boon\n",
    "    pattern = r'\\*{3} START OF (?:THE|THIS) PROJECT GUTENBERG EBOOK [\\r\\n \\w]+ \\*{3}((?s).*)\\*{3} END OF (?:THE|THIS) PROJECT GUTENBERG EBOOK [\\r\\n \\w]+ \\*{3}'\n",
    "    \n",
    "    # Apply regex to the content\n",
    "    content = re.findall(pattern, text)[0]\n",
    "    \n",
    "    # Replace Windows newline characters ('\\r\\n') with standard newline characters \n",
    "    return re.sub(r'\\r\\n', '\\n', content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11027fd-b4c3-4b03-aec3-067b2b1ed7b6",
   "metadata": {},
   "source": [
    "### ‚úÖ Testing on The Great Gatsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc360f25-8674-4ad9-bb5b-2e08ea78d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        \\n\\n\\t\\t\\t   The Great Gatsby\\n\\t\\t\\t\\t  by\\n\\t\\t\\t F. Scott Fitzgerald\\n\\n\\n                           Table of Contents\\n\\nI\\nII\\nIII\\nIV\\nV\\nVI\\nVII\\nVIII\\nIX\\n\\n\\n                              Once again\\n                                  to\\n                                 Zelda\\n\\n  Then wear the gold hat, if that will move her;\\n  If you can bounce high, bounce for her too,\\n  Till she cry ‚ÄúLover, gold-hatted, high-bouncing lover,\\n  I must have you!‚Äù\\n\\n  Thomas Parke d‚ÄôInvilliers\\n\\n\\n                                  I\\n\\nIn my younger and more vulnerable years my father gave me some advice\\nthat I‚Äôve been turning over in my mind ever since.\\n\\n‚ÄúWhenever you feel like criticizing anyone,‚Äù he told me, ‚Äújust\\nremember that all the people in this world haven‚Äôt had the advantages\\nthat you‚Äôve had.‚Äù\\n\\nHe didn‚Äôt say any more, but we‚Äôve always been unusually communicative\\nin a reserved way, and I understood that he meant a great deal more\\nthan that. In consequence, I‚Äôm inclined to reserve all judgements, a\\nhabit that has opened up many curious natures to me and also made me\\nthe victim of not a few veteran bores. The abnormal mind is quick to\\ndetect and attach itself to this quality when it appears in a normal\\nperson, and so it came about that in college I was unjustly accused of\\nbeing a politician, because I was privy to the secret griefs of wild,\\nunknown men. Most of the confidences were unsought‚Äîfrequently I have\\nfeigned sleep, preoccupation, or a hostile levity when I realized by\\nsome unmistakable sign that an intimate revelation was quivering on\\nthe horizon; for the intimate revelations of young men, or at least\\nthe terms in which they express them, are usually plagiaristic and\\nmarred by obvious suppressions. Reserving judgements is a matter of\\ninfinite hope. I am still a little afraid of missing something if I\\nforget that, as my father snobbishly suggested, and I snobbishly\\nrepeat, a sense of the fundamental decencies is parcelled out\\nunequally at birth.\\n\\nAnd, after boasting this way of my toleran'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great Gatsby URL provided in Plain Text UTF-8\n",
    "great_gatsby = get_book(\"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\")\n",
    "\n",
    "# First 2000 characters of The Great Gatsby\n",
    "great_gatsby[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917226b-4a4a-415a-a509-536366293f65",
   "metadata": {},
   "source": [
    "## ü™ì 2) Tokenizing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7975c4-1abc-4dfa-bff8-246ae8d462ea",
   "metadata": {},
   "source": [
    "I **tokenize** the text by implementing the function `tokenize`, which takes in a string, `book_string`, and returns a **list of the tokens** (words, numbers, and all punctuation) in the book such that:\n",
    "\n",
    "* The start of every paragraph is represented in the list with the single character `'\\x02'` (standing for START).\n",
    "* The end of every paragraph is represented in the list with the single character `'\\x03'` (standing for STOP).\n",
    "* Tokens include *no* whitespace.\n",
    "* Two or more newlines count as a paragraph break, and whitespace (e.g. multiple newlines) between two paragraphs of text do not appear as tokens.\n",
    "* All punctuation marks count as tokens, even if they are uncommon (e.g. `'@'`, `'+'`, and `'%'` are all valid tokens).\n",
    "\n",
    "For example, consider the following excerpt. (The first sentence is at the end of a larger paragraph, and the second sentence is at the start of a longer paragraph.)\n",
    "```\n",
    "...\n",
    "My phone's dead.\n",
    "\n",
    "I didn't get your call!!\n",
    "...\n",
    "```\n",
    "Tokenizes to:\n",
    "```py\n",
    "[...\n",
    "'My', 'phone', \"'\", 's', 'dead', '.', '\\x03', '\\x02', 'I', 'didn', \"'\", 't', 'get', 'your', 'call', '!', '!'\n",
    "...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6c6487a-33cd-4be9-a49b-5e206c249040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(book_string):\n",
    "    \n",
    "    # Remove any leading or trailing spaces from the original \n",
    "    book_string = '\\x02'+book_string.strip()+'\\x03'\n",
    "    \n",
    "    # Replace the beginning of two or more consecutive newlines \n",
    "    book_string = re.sub('^\\n{2,}', '\\x02', book_string)\n",
    "    \n",
    "    # Replace the end of two or more consecutive newlines \n",
    "    book_string = re.sub('\\n{2,}$', '\\x03', book_string)\n",
    "    \n",
    "    # Replaces any other occurrence of two or more consecutive newlines \n",
    "    book_string = re.sub('\\n{2,}', '\\x03\\x02', book_string)\n",
    "    \n",
    "    # Matches any alpha or non-alpha characters\n",
    "    pattern = r'[A-Za-z]+|[^\\s\\d\\w]|\\x03|\\x02'\n",
    "    \n",
    "    # Returns list containing individual tokens extracted from the book_string\n",
    "    return re.findall(pattern, book_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490a18e-8deb-4d68-86b2-8b89ded456dc",
   "metadata": {},
   "source": [
    "### ‚úÖ Testing: tokenize() on The Great Gatsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5816c8d8-af42-49f3-bbbe-ddfe137980df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\x02', 'The', 'Great', 'Gatsby', 'by', 'F', '.', 'Scott',\n",
       "       'Fitzgerald', '\\x03', '\\x02', 'Table', 'of', 'Contents', '\\x03',\n",
       "       '\\x02', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX',\n",
       "       '\\x03', '\\x02', 'Once', 'again', 'to', 'Zelda', '\\x03', '\\x02',\n",
       "       'Then', 'wear', 'the', 'gold', 'hat', ',', 'if', 'that', 'will',\n",
       "       'move', 'her', ';', 'If', 'you', 'can', 'bounce', 'high', ',',\n",
       "       'bounce', 'for', 'her', 'too', ',', 'Till', 'she', 'cry', '‚Äú',\n",
       "       'Lover', ',', 'gold', '-', 'hatted', ',', 'high', '-', 'bouncing',\n",
       "       'lover', ',', 'I', 'must', 'have', 'you', '!', '‚Äù', '\\x03', '\\x02',\n",
       "       'Thomas', 'Parke', 'd', '‚Äô', 'Invilliers', '\\x03', '\\x02', 'I',\n",
       "       '\\x03', '\\x02', 'In', 'my', 'younger', 'and', 'more', 'vulnerable',\n",
       "       'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that',\n",
       "       'I', '‚Äô', 've', 'been', 'turning', 'over', 'in', 'my', 'mind',\n",
       "       'ever', 'since', '.', '\\x03', '\\x02', '‚Äú', 'Whenever', 'you',\n",
       "       'feel', 'like', 'criticizing', 'anyone', ',', '‚Äù', 'he', 'told',\n",
       "       'me', ',', '‚Äú', 'just', 'remember', 'that', 'all', 'the', 'people',\n",
       "       'in', 'this', 'world', 'haven', '‚Äô', 't', 'had', 'the',\n",
       "       'advantages', 'that', 'you', '‚Äô', 've', 'had', '.', '‚Äù', '\\x03',\n",
       "       '\\x02', 'He', 'didn', '‚Äô', 't', 'say', 'any', 'more', ',', 'but',\n",
       "       'we', '‚Äô', 've', 'always', 'been', 'unusually', 'communicative',\n",
       "       'in', 'a', 'reserved', 'way', ',', 'and', 'I', 'understood',\n",
       "       'that', 'he', 'meant', 'a', 'great', 'deal', 'more', 'than',\n",
       "       'that', '.', 'In', 'consequence', ',', 'I', '‚Äô', 'm', 'inclined',\n",
       "       'to', 'reserve', 'all', 'judgements'], dtype='<U17')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the Great Gatsby\n",
    "tokenized_gatsby = tokenize(great_gatsby)\n",
    "\n",
    "# First 200 tokens\n",
    "np.array(tokenized_gatsby)[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d59667-f7c7-407f-8dcd-c8ce607eef82",
   "metadata": {},
   "source": [
    "## Creating Baseline Language Models üìï"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e819cb1-7c06-4422-87c2-e89543120d4c",
   "metadata": {},
   "source": [
    "Sentences are built from tokens, and the likelihood that a token occurs where it does depends on the tokens before it. This points to using **conditional probability** to compute $P(w)$. That is, we can write:\n",
    "\n",
    "$$\n",
    "P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})\n",
    "$$  \n",
    "Using **chain rule** for probabilities.\n",
    "\n",
    "**Example:** \n",
    "\n",
    "<center><code>'when I drink Coke I smile'</code></center>\n",
    "    \n",
    "The probability that it occurs, according the the chain rule, is\n",
    "\n",
    "$$\n",
    "P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I})\\cdot P(\\text{Coke | when I drink}) \\cdot P(\\text{I | when I drink Coke}) \\cdot P(\\text{smile | when I drink Coke I})\n",
    "$$\n",
    "\n",
    "That is, the probability that the sentence occurs is the product of the probability that each subsequent token follows the tokens that came before. For example, the probability $P(\\text{Coke | when I drink})$ is likely pretty high, as Coke is something that you drink. The probability $P(\\text{pizza | when I drink})$ is likely low, because pizza is not something that you drink.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9f0fe-9d15-420c-a242-9037ad3acd42",
   "metadata": {},
   "source": [
    "## üé≤ Creating a Unifrom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7990e-a5ba-4ee8-b57d-6c2233d6cc01",
   "metadata": {},
   "source": [
    "A uniform language model is one in which each **unique** token is equally likely to appear in any position, unconditional of any other information. In other words, in a uniform language model, the probability assigned to each token is **1 over the total number of unique tokens in the corpus**.\n",
    "\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "The example corpus above has 14 **unique** tokens. This means that I'd have $P(\\text{\\x02}) = \\frac{1}{14}$, $P(\\text{when}) = \\frac{1}{14}$, and so on. Specifically, in this example, **the Series that `train` returns should contain the following values**:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{14}$ |\n",
    "| `'when'` | $\\frac{1}{14}$ |\n",
    "| `'I'` | $\\frac{1}{14}$ |\n",
    "| `'eat'` | $\\frac{1}{14}$ |\n",
    "| `'pizza'` | $\\frac{1}{14}$ |\n",
    "| `','` | $\\frac{1}{14}$ |\n",
    "| `'smile'` | $\\frac{1}{14}$ |\n",
    "| `'but'` | $\\frac{1}{14}$ |\n",
    "| `'drink'` | $\\frac{1}{14}$ |\n",
    "| `'Coke'` | $\\frac{1}{14}$ |\n",
    "| `'my'` | $\\frac{1}{14}$ |\n",
    "| `'stomach'` | $\\frac{1}{14}$ |\n",
    "| `'hurts'` | $\\frac{1}{14}$ |\n",
    "| `'\\x03'` | $\\frac{1}{14}$ |\n",
    "\n",
    "#### Unifrom Class:\n",
    "\n",
    "* The `__init__` constructor: when you instantiate an LM object, I pass in the \"training corpus\" on which my model will be trained. The `train` method uses that data to create a model which is saved in the `mdl` attribute. \n",
    "* The `train` method takes in a list of tokens and outputs a language model. **This language model is represented as a `Series`, whose index consists of tokens and whose values are the probabilities that the tokens occur.** \n",
    "* The `probability` method takes in a sequence of tokens and returns the probability that this sequence occurs under the language model.\n",
    "* The `sample` method takes in a positive integer `M` and generates a string made up of `M` tokens using the language model. **This method generates random sentences!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5dc2c6e0-5156-428b-b96f-cd8eb8d6983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformLM(object):\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        # Constructor method\n",
    "        # Initializes the class instance and trains the language model.\n",
    "        # 'tokens' is a list of words used to train the language model.\n",
    "        self.mdl = self.train(tokens)\n",
    "        \n",
    "    def train(self, tokens):\n",
    "        # Training method\n",
    "        # Creates a uniform language model from the provided list of tokens.\n",
    "        # 'tokens' is a list of words used to build the vocabulary of the model.\n",
    "        unique_tokens = pd.Series(tokens).unique()\n",
    "        # Calculate the uniform probability for each unique token and create a pandas Series.\n",
    "        return pd.Series(np.full(len(unique_tokens), 1/len(unique_tokens)), index=unique_tokens)\n",
    "    \n",
    "    def probability(self, words):\n",
    "        # Probability calculation method\n",
    "        # Calculates the probability of a sequence of words given the trained language model.\n",
    "        try:\n",
    "            # Convert 'words' to a list to ensure we can access elements by index.\n",
    "            words = list(words)\n",
    "            # Calculate the probability of the sequence by taking the product of individual word probabilities.\n",
    "            prob = np.prod(self.mdl.loc[words])\n",
    "        except KeyError:\n",
    "            # If any word in the sequence is not present in the language model's vocabulary,\n",
    "            # set the probability to 0 indicating that the sequence is not recognized.\n",
    "            prob = 0\n",
    "        return prob \n",
    "        \n",
    "    def sample(self, M):\n",
    "        # Sampling method\n",
    "        # Generates a random sequence of 'M' words from the language model.\n",
    "        # 'M' is the number of words to be sampled.\n",
    "        return ' '.join(self.mdl.sample(M, replace=True).index)\n",
    "        # Generate a random sample of 'M' words with replacement from the language model's vocabulary.\n",
    "        # Join the sampled words into a string with spaces between them and return the string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc62439-52df-49e6-ba0a-ea0e00232f47",
   "metadata": {},
   "source": [
    "### ‚úÖ Testing: UnifromML() on The Great Gatsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28501eaf-34ee-4520-9619-f374b2c916b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drum argue waiter Haven His Probably eyebrow dates Schraeders Montana powdered representing emergency brooded Backhyssons bathrooms support betting composed Platonic rumour Taking pulpless Flushing dog eager visitors wheel upright desolate afford despairing painting excellence bles getting pomp infantry scalloped destiny riding frequently preyed floating mistake facet February Americans ran tonic Tom wildly tumult glamour rounds orange Never Another event complained slump De breath gambler Be fluttered realize ridges judging entangled paralysed bles smart ceiling objects rubber grey firmly trees Inside stifling Eckleburg tearing Eggers permanently laudable events Webster eyes explicable each retribution star tactlessly fare meanwhile can except rapidly fix'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenized_gatsby\n",
    "\n",
    "# Initilize and train Unifrom model on given tokens\n",
    "unif = UniformLM(tokens)\n",
    "\n",
    "# Generates a random sequence of 100 words from the language model.\n",
    "unif.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27cddaa-d3f7-401b-9fca-4d501721e802",
   "metadata": {},
   "source": [
    "## ü™á Creating a Uni-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572c3f7-ea27-44a4-861c-db6c1b7cb6f0",
   "metadata": {},
   "source": [
    "A unigram language model is one in which the **probability assigned to a token is equal to the proportion of tokens in the corpus that are equal to said token**. That is, the probability distribution associated with a unigram language model is just the empirical distribution of tokens in the corpus. \n",
    "\n",
    "Let's understand how probabilities are assigned to tokens using our example corpus from before.\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokenize(corpus)\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    "```\n",
    "\n",
    "Here, there are 19 total tokens. 3 of them are equal to `'I'`, so $P(\\text{I}) = \\frac{3}{19}$. Here, the Series that `train` returns should contain the following values:\n",
    "\n",
    "| Token | Probability |\n",
    "| --- | --- |\n",
    "| `'\\x02'` | $\\frac{1}{19}$ |\n",
    "| `'when'` | $\\frac{2}{19}$ |\n",
    "| `'I'` | $\\frac{3}{19}$ |\n",
    "| `'eat'` | $\\frac{1}{19}$ |\n",
    "| `'pizza'` | $\\frac{1}{19}$ |\n",
    "| `','` | $\\frac{3}{19}$ |\n",
    "| `'smile'` | $\\frac{1}{19}$ |\n",
    "| `'but'` | $\\frac{1}{19}$ |\n",
    "| `'drink'` | $\\frac{1}{19}$ |\n",
    "| `'Coke'` | $\\frac{1}{19}$ |\n",
    "| `'my'` | $\\frac{1}{19}$ |\n",
    "| `'stomach'` | $\\frac{1}{19}$ |\n",
    "| `'hurts'` | $\\frac{1}{19}$ |\n",
    "| `'\\x03'` | $\\frac{1}{19}$ |\n",
    "\n",
    "As before, the `probability` method should take in a tuple and return its probability, using the probabilities stored in `mdl`. For instance, suppose the input tuple is `('when', 'I', 'drink', 'Coke', 'I', 'smile')`. Then,\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I}) \\cdot P(\\text{drink}) \\cdot P(\\text{Coke}) \\cdot P(\\text{I}) \\cdot P(\\text{smile}) = \\frac{2}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19} \\cdot \\frac{1}{19} \\cdot \\frac{3}{19} \\cdot \\frac{1}{19}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78844432-963f-4a8f-9fea-45d6bbcef621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramLM(object):\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        # Constructor method\n",
    "        # Initializes the class instance and trains the unigram language model.\n",
    "        # 'tokens' is a list of words used to train the language model.\n",
    "        self.mdl = self.train(tokens)\n",
    "    \n",
    "    def train(self, tokens):\n",
    "        # Training method\n",
    "        # Creates a unigram language model from the provided list of tokens.\n",
    "        # 'tokens' is a list of words used to build the vocabulary of the model.\n",
    "        tokens = pd.Series(tokens)\n",
    "        # Convert 'tokens' to a pandas Series to perform value counting.\n",
    "        return tokens.value_counts().apply(lambda x: x/len(tokens))\n",
    "        # Calculate the frequency (probability) of each token and normalize it by dividing by the total number of tokens.\n",
    "        # Create a pandas Series where the index represents each unique token and the values represent their probabilities.\n",
    "\n",
    "    def probability(self, words):\n",
    "        # Probability calculation method\n",
    "        # Calculates the probability of a sequence of words given the trained unigram language model.\n",
    "        try:\n",
    "            # Convert 'words' to a list to ensure we can access elements by index.\n",
    "            words = list(words)\n",
    "            # Calculate the probability of the sequence by taking the product of individual word probabilities.\n",
    "            prob = np.prod(self.mdl.loc[words])\n",
    "        except KeyError:\n",
    "            # If any word in the sequence is not present in the language model's vocabulary,\n",
    "            # set the probability to 0 indicating that the sequence is not recognized.\n",
    "            prob = 0\n",
    "        return prob\n",
    "        \n",
    "    def sample(self, M):\n",
    "        # Sampling method\n",
    "        # Generates a random sequence of 'M' words from the language model.\n",
    "        # 'M' is the number of words to be sampled.\n",
    "        return ' '.join(self.mdl.sample(M, replace=True).index)\n",
    "        # Generate a random sample of 'M' words with replacement from the language model's vocabulary.\n",
    "        # Join the sampled words into a string with spaces between them and return the string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47530c4-2847-4c15-a3ae-2e9e1543db6c",
   "metadata": {},
   "source": [
    "### ‚úÖ Testing: UnigramML() on The Great Gatsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c4542d1-cdd6-4d8c-b816-878f217d82cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chill bug bumped lie spotted sleeves astounded actress fenders dull shots lots Ahead imagine America telephoned crickets unable Now invitations attempting fantastic cheekbone sleep cornices toiled inventions finished caf humming further capable burns moving generations grocery shall charm Another suite workmen happens state vague centre tangle saloons always roaring milky kinds Goddard flood rounds silence sports embroidered date dispute inessential sixteen subdued tag dissimilarity period distaste incident July determinedly substitute sold alongside raining reserve marvellous Bay dates declaration Dumbell signed roads carrying imagination bug cr hollow groceries breathlessly countered postern purposeless cleaned a Hulking policeman mean inquisitions border group spidery'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenized_gatsby\n",
    "\n",
    "# Initilize and train Unifrom model on given tokens\n",
    "unif = UnigramLM(tokens)\n",
    "\n",
    "# Generates a random sequence of 100 words from the language model.\n",
    "unif.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9475a-4145-479e-be70-5355a0243cd3",
   "metadata": {},
   "source": [
    "## üé≥ Creating NGram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e39e4-2340-4ffc-98b9-a0e8062c9c08",
   "metadata": {},
   "source": [
    "The N-Gram language model relies on the assumption that only nearby tokens matter. Specifically, it assumes that the probability that a token occurs depends only on the previous $N-1$ tokens, rather than all previous tokens. That is:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "In an N-Gram language model, there is a hyperparameter that we get to choose when creating the model, $N$. For any $N$, the resulting N-Gram model looks at the previous $N-1$ tokens when computing probabilities. (Note that the unigram model you built in Question 4 is really an N-Gram model with $N=1$, since it looked at 0 previous tokens when computing probabilities.)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Example: Trigram Model\n",
    "\n",
    "When $N=3$, it's a \"trigram\" model. Such a model looks at the previous $N-1 = 2$ tokens when computing probabilities.\n",
    "\n",
    "Consider the tuple `('when', 'I', 'drink', 'Coke', 'I', 'smile')`, corresponding to the sentence `'when I drink Coke I smile'`. Under the trigram model, the probability of this sentence is computed as follows:\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I}) \\cdot P(\\text{Coke | I drink}) \\cdot P(\\text{I | drink Coke}) \\cdot P(\\text{smile | Coke I})$$\n",
    "\n",
    "The trigram model doesn't consider the beginning of the sentence when computing the probability that the sentence ends in `'smile'`.\n",
    "\n",
    "#### N-Grams\n",
    "\n",
    "Both when working with a training corpus and when implementing the `probability` method to compute the probabilities of other sentences, I use  \"chunks\" of $N$ tokens at a time.\n",
    "\n",
    "**Definition:** The **N-Grams of a text** are a list of tuples containing sliding windows of length $N$.\n",
    "\n",
    "For instance, the trigrams in the sentence `'when I drink Coke I smile'` are:\n",
    "\n",
    "```py\n",
    "[('when', 'I', 'drink'), ('I', 'drink', 'Coke'), ('drink', 'Coke', 'I'), ('Coke', 'I', 'smile')]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Computing N-Gram Probabilities\n",
    "\n",
    "Notice in our trigram model above, I computed $P(\\text{when I drink Coke I smile})$ as being the product of several conditional probabilities. These conditional probabilities are the result of **training** our N-Gram model on a training corpus.\n",
    "\n",
    "To train an N-Gram model, I compute a conditional probability for every $N$-token sequence in the corpus. For instance, for every 3-token sequence $w_1, w_2, w_3$, I must compute $P(w_3 | w_1, w_2)$. To do so, I use:\n",
    "\n",
    "$$P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram sequence $w_1, w_2, w_3$ in the training corpus and $C(w_1, w_2)$ is the number of occurrences of the bigram sequence  $w_1, w_2$ in the training corpus. (Technical note: the probabilities that I compute using the ratios of counts are _estimates_ of the true conditional probabilities of N-Grams in the population of corpuses from which our corpus was drawn.)\n",
    "\n",
    "In general, for any $N$, conditional probabilities are computed by dividing the counts of N-Grams by the counts of the (N-1)-Grams they follow. \n",
    "\n",
    "<br>\n",
    "\n",
    "### The `NGramLM` Class\n",
    "\n",
    "The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and a positive integer `N`, specifying the N in N-grams. This parameter is stored in an attribute `N`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-Grams (recall from above, an N-Gram is a **tuple** of length N). This list of N-Grams is then passed to the `train` method to train the N-Gram model.\n",
    "1. While the `train` method still creates a language model (in this case, an N-Gram model) and stores it in the `mdl` attribute, this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`, containing the N-Grams found in the text.\n",
    "    - `'n1gram'`, containing the (N-1)-Grams upon which the N-Grams in `ngram` are built.\n",
    "    - `'prob'`, containing the probabilities of each N-Gram in `ngram`.\n",
    "1. The `NGramLM` class has an attribute `prev_mdl` that stores an (N-1)-Gram language model over the same corpus (which in turn will store an (N-2)-Gram language model over the same corpus, and so on). This is necessary to compute the probability that a word occurs at the start of a text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9570c4d-a198-40fd-b011-233a99a66bcb",
   "metadata": {},
   "source": [
    "N-Gram LM consists of probabilities of the form\n",
    "\n",
    "$$P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "Which can be estimated by:  \n",
    "\n",
    "$$\\frac{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1}, w_n)}{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1})}$$\n",
    "\n",
    "for every N-Gram that occurs in the corpus. To illustrate, consider again the following example corpus:\n",
    "\n",
    "```py\n",
    ">>> corpus = 'when I eat pizza, I smile, but when I drink Coke, my stomach hurts'\n",
    ">>> tokens = tokenize(corpus)\n",
    ">>> tokens\n",
    "['\\x02', 'when', 'I', 'eat', 'pizza', ',', 'I', 'smile', ',', 'but', 'when', 'I', 'drink', 'Coke', ',', 'my', 'stomach', 'hurts', '\\x03']\n",
    ">>> pizza_model = NGrams(3, tokens)\n",
    "```\n",
    "\n",
    "Here, `pizza_model.train` must compute $P(\\text{I | \\x02 when})$, $P(\\text{eat | when I})$, $P(\\text{pizza | I eat})$, and so on, until $P(\\text{\\x03 | stomach hurts})$.\n",
    "\n",
    "To compute $P(\\text{eat | when I})$, I find the number of occurrences of `'when I eat'` in the training corpus, and divide it by the number of occurrences of `'when I'` in the training corpus. `'when I eat'` occurred exactly once in the training corpus, while `'when I'` occurred twice, so,\n",
    "\n",
    "$$P(\\text{eat | when I}) = \\frac{C(\\text{when I eat})}{C(\\text{when I})} = \\frac{1}{2}$$\n",
    "\n",
    "To store the conditional probabilities of all N-Grams, I use a DataFrame with three columns, like so:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ngram</th>\n",
    "      <th>n1gram</th>\n",
    "      <th>prob</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>(when, I, drink)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>(when, I, eat)</td>\n",
    "      <td>(when, I)</td>\n",
    "      <td>0.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>(,, but, when)</td>\n",
    "      <td>(,, but)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>(,, I, smile)</td>\n",
    "      <td>(,, I)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>(I, smile, ,)</td>\n",
    "      <td>(I, smile)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>(,, my, stomach)</td>\n",
    "      <td>(,, my)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>(but, when, I)</td>\n",
    "      <td>(but, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>(\u0002, when, I)</td>\n",
    "      <td>(\u0002, when)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>(stomach, hurts, \u0003)</td>\n",
    "      <td>(stomach, hurts)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>(Coke, ,, my)</td>\n",
    "      <td>(Coke, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>(eat, pizza, ,)</td>\n",
    "      <td>(eat, pizza)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>(I, drink, Coke)</td>\n",
    "      <td>(I, drink)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>(my, stomach, hurts)</td>\n",
    "      <td>(my, stomach)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>(pizza, ,, I)</td>\n",
    "      <td>(pizza, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>(I, eat, pizza)</td>\n",
    "      <td>(I, eat)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>(drink, Coke, ,)</td>\n",
    "      <td>(drink, Coke)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>(smile, ,, but)</td>\n",
    "      <td>(smile, ,)</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "The row at position **1** in the above table shows that the probability of the trigram `('when', 'I', 'eat')` conditioned on the bigram `('when', 'I')` is 0.5, as we computed above. Note that many of the above conditional probabilities are equal to 1 because many trigrams and their corresponding bigrams each appeared only once, and $\\frac{1}{1} = 1$. Note that `'\\x02'` and `'\\x03'` appear as spaces above, such as in row **7**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "12e2b359-afff-4f2d-8744-7e50d2a2818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM(object):\n",
    "    \n",
    "    def __init__(self, N, tokens):\n",
    "        # Constructor method\n",
    "        # Initializes the class instance for an N-gram language model.\n",
    "        # 'N' is the order of the N-gram model.\n",
    "        # 'tokens' is a list of words used to train the language model.\n",
    "        \n",
    "        self.N = N\n",
    "        # Store the N-gram order in the class attribute 'N'.\n",
    "        \n",
    "        ngrams = self.create_ngrams(tokens)\n",
    "        self.tok = tokens\n",
    "        self.ngrams = ngrams\n",
    "        self.mdl = self.train(ngrams)\n",
    "        # Create N-grams from the provided tokens and train the N-gram language model.\n",
    "        \n",
    "        if N < 2:\n",
    "            raise Exception('N must be greater than 1')\n",
    "        # Check if N is less than 2, which means the model is not valid (N should be 2 or greater).\n",
    "        \n",
    "        elif N == 2:\n",
    "            self.prev_mdl = UnigramLM(tokens)\n",
    "        else:\n",
    "            self.prev_mdl = NGramLM(N-1, tokens)\n",
    "        # If N is 2, create a UnigramLM as a previous model (prev_mdl).\n",
    "        # Otherwise, create an (N-1)-gram language model as a previous model (prev_mdl).\n",
    "        \n",
    "    def create_ngrams(self, tokens):\n",
    "        # N-gram creation method\n",
    "        # Creates N-grams from a list of tokens.\n",
    "        # 'tokens' is a list of words used to build the N-grams.\n",
    "        \n",
    "        result = []\n",
    "        right_pointer = self.N\n",
    "        left_pointer = 0\n",
    "        \n",
    "        while right_pointer <= len(tokens):\n",
    "            # Create N-gram by taking a slice of tokens from left_pointer to right_pointer.\n",
    "            result.append(tuple(tokens[left_pointer:right_pointer]))\n",
    "            left_pointer += 1\n",
    "            right_pointer += 1\n",
    "        \n",
    "        return result\n",
    "        # Return the list of N-grams created from the tokens.\n",
    "    \n",
    "    def create_ngrams_N(self, n, tokens):\n",
    "        # N-gram creation method for a specific value of N\n",
    "        # Creates N-grams from a list of tokens.\n",
    "        # 'n' is the specific N-gram order.\n",
    "        # 'tokens' is a list of words used to build the N-grams.\n",
    "        \n",
    "        res = []\n",
    "        right = n\n",
    "        left = 0\n",
    "\n",
    "        while right <= len(tokens):\n",
    "            # Create N-gram by taking a slice of tokens from left to right.\n",
    "            res.append(tuple(tokens[left:right]))\n",
    "            left += 1\n",
    "            right += 1\n",
    "        \n",
    "        return res\n",
    "        # Return the list of N-grams created from the tokens.\n",
    "        \n",
    "    def train(self, ngrams):\n",
    "        # N-gram training method\n",
    "        # Calculates probabilities for each N-gram in the training data.\n",
    "        # 'ngrams' is the list of N-grams used to train the model.\n",
    "        \n",
    "        # Create pandas Series from N-grams to facilitate computations.\n",
    "        ser_ngram = pd.Series(ngrams, name=\"ngram\")\n",
    "        # Create (N-1)-grams from the tokens.\n",
    "        n1gram_create = self.create_ngrams_N(self.N-1, self.tok)\n",
    "        ser_n1gram = pd.Series(n1gram_create, name=\"n1gram\")\n",
    "\n",
    "        # Merge N-grams and (N-1)-grams on common elements to calculate probabilities.\n",
    "        merge_ngram_n1gram = pd.merge(ser_ngram, ser_n1gram, left_index=True, right_index=True)\n",
    "        # Calculate counts of each (N-1)-gram to use as denominators for probabilities.\n",
    "        denom_col = merge_ngram_n1gram.groupby(\"n1gram\").transform('count')\n",
    "        # Calculate counts of each N-gram to use as numerators for probabilities.\n",
    "        numer_col = merge_ngram_n1gram.groupby(\"ngram\").transform('count')\n",
    "        # Add numerator and denominator columns to the DataFrame.\n",
    "        merge_ngram_n1gram[\"numerator\"] = numer_col\n",
    "        merge_ngram_n1gram[\"denominator\"] = denom_col\n",
    "        # Calculate probabilities for each N-gram based on the counts.\n",
    "        merge_ngram_n1gram[\"prob\"] = merge_ngram_n1gram[\"numerator\"] / merge_ngram_n1gram[\"denominator\"]\n",
    "        \n",
    "        # Drop the intermediate columns and keep only unique N-grams with their probabilities.\n",
    "        return merge_ngram_n1gram.drop(columns=[\"numerator\", \"denominator\"]).drop_duplicates(keep='first')\n",
    "    \n",
    "    def probability(self, words):\n",
    "        # Probability calculation method\n",
    "        # It calculates the probability of a sequence of words given the trained N-gram language model.\n",
    "        # 'words' is the sequence of words for which the probability is calculated.\n",
    "        \n",
    "        input_words = ' '.join(words)\n",
    "        token_words = ' '.join(self.tok)\n",
    "\n",
    "        if input_words not in token_words:\n",
    "            # If the input sequence is not present in the training data, return 0 probability.\n",
    "            return 0\n",
    "        \n",
    "        final_prob = 1\n",
    "        \n",
    "        initial_prob = []\n",
    "        for i in range(self.N-1, 0, -1):\n",
    "            if i == 1:\n",
    "                initial_prob.append(words[0:i][0])\n",
    "            else:\n",
    "                initial_prob.append(words[0:i])\n",
    "        # Create a list of (N-1)-grams that precede the input word sequence.\n",
    "\n",
    "        list_grams = self.create_ngrams(words)\n",
    "        # Create N-grams from the input word sequence.\n",
    "        \n",
    "        for gram in list_grams: \n",
    "            if gram not in list(self.mdl['ngram'].values):\n",
    "                # If any N-gram in the input word sequence is not present in the model, return 0 probability.\n",
    "                return 0\n",
    "            else:\n",
    "                final_prob *= self.mdl[self.mdl['ngram'] == gram]['prob'].iloc[0]\n",
    "                # Multiply the probabilities of each N-gram in the input word sequence.\n",
    "                # Access the corresponding probability from the trained N-gram model.\n",
    "\n",
    "        current = self\n",
    "        for gram in initial_prob:\n",
    "            # Loop through the list of (N-1)-grams in reverse order (from largest to smallest).\n",
    "            current = current.prev_mdl\n",
    "            # Traverse to the previous model for each (N-1)-gram.\n",
    "            current_table = current.mdl\n",
    "            # Access the probability table of the previous model.\n",
    "\n",
    "            if isinstance(gram, str):\n",
    "                # If the (N-1)-gram is a single word (unigram), access its probability from the unigram model.\n",
    "                final_prob *= pd.DataFrame(current_table).loc[gram].iloc[0]\n",
    "            else:\n",
    "                # If the (N-1)-gram is not a unigram, access its probability from the N-1 gram model.\n",
    "                final_prob *= current_table[current_table[\"ngram\"] == gram][\"prob\"].iloc[0]\n",
    "        # Multiply probabilities of (N-1)-grams that precede the input word sequence.\n",
    "\n",
    "        return final_prob\n",
    "        # Return the final calculated probability of the input word sequence.\n",
    "        \n",
    "    def sample(self, M):\n",
    "        # Sampling method\n",
    "        # It generates a random sequence of 'M' words from the language model.\n",
    "        # 'M' is the number of words to be sampled.\n",
    "        \n",
    "        sample_words = ['\\x02']\n",
    "        # Initialize the list of sampled words with a special start symbol '\\x02'.\n",
    "        \n",
    "        for i in range(1, self.N):\n",
    "            # Loop from 1 to N-1 to generate initial (N-1)-grams.\n",
    "            current = self.N - i\n",
    "            prev = self\n",
    "            \n",
    "            for i in range(current, 1, -1):\n",
    "                # Traverse to the previous model for each (N-1)-gram.\n",
    "                prev = prev.prev_mdl\n",
    "            \n",
    "            # Access the probability table for the (N-1)-gram model.\n",
    "            probability = prev.mdl[np.where(prev.mdl['n1gram'].apply(str) == str(tuple(sample_words)), True, False)].drop_duplicates(keep='first')\n",
    "            \n",
    "            if probability.shape[0] == 0 or probability.shape[1] == 0:\n",
    "                sample_words.append('\\x03')\n",
    "                # If no probabilities found for the (N-1)-gram, add the end symbol '\\x03'.\n",
    "            else:\n",
    "                # Sample the next word using the probabilities from the (N-1)-gram model.\n",
    "                smalls = np.random.choice(probability['ngram'], p=probability['prob'])\n",
    "                sample_words.append(smalls[-1])\n",
    "                # Append the last word of the sampled (N-1)-gram to the sampled words list.\n",
    "        \n",
    "        for j in range(self.N, M+1):\n",
    "            # Loop from N to M (inclusive) to generate N-grams for the rest of the sequence.\n",
    "            probability = prev.mdl[np.where(prev.mdl['n1gram'].apply(str) == str(tuple(sample_words[-self.N+1:])), True, False)].drop_duplicates(keep='first')\n",
    "            \n",
    "            if probability.shape[0] == 0 or probability.shape[1] == 0:\n",
    "                sample_words.append('\\x03')\n",
    "                # If no probabilities found for the N-gram, add the end symbol '\\x03'.\n",
    "            else:\n",
    "                # Sample the next word using the probabilities from the N-gram model.\n",
    "                words = np.random.choice(probability['ngram'], p=probability['prob'])\n",
    "                sample_words.append(np.random.choice(probability['ngram'], p=probability['prob'])[-1])\n",
    "                # Append the last word of the sampled N-gram to the sampled words list.\n",
    "        \n",
    "        if sample_words[-1] != '\\x03':\n",
    "            sample_words[-1] = '\\x03'\n",
    "            # Replace the last word with the end symbol '\\x03'.\n",
    "        \n",
    "        return ' '.join(sample_words)\n",
    "        # Join the sampled words into a string with spaces between them and return the string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d772fd-00f8-409f-a65a-a89daf106251",
   "metadata": {},
   "source": [
    "### ‚úÖ Testing: N() on The Great Gatsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a954b3f-847b-4f41-be50-216346ffb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGramLM(3, tokenized_gatsby)\n",
    "ngram_series_representation = ngram.create_ngrams(tokens)\n",
    "ngram_dataframe_representation = ngram.mdl\n",
    "ngram_model = ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c31153ad-db35-44a8-9161-b7244281de1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x02 ‚Äú And your house some afternoon and Long Distance said Chicago was calling up a nice restaurant here , ‚Äù mumbled Miss Baedeker ? ‚Äù \\x03 \\x02 Cody was fifty years old , ‚Äù said Wilson , nodding into the twilight , but a whole floor of the youth and , even through his punctilious manner in the room . With Jordan ‚Äô s a regular Belasco . It ‚Äô s voice calling a taxi go up Gatsby ‚Äô s hospitality and so it came about that in a low husky sob , and Doctor Webster Civet , who had arrived , they were selling something : bonds or insurance or automobiles . They ‚Äô re Jordan Baker . ( ‚Äú Rot - Gut ‚Äù ) Ferret and the adventitious authority of his destiny , to be brought to him , but Wilson wouldn ‚Äô t reached West Egg . ‚Äù \\x03 \\x02 ‚Äú You don ‚Äô t get mixed up in his car . ‚Äù \\x03 \\x02 ‚Äú Does the gasoline affect his nose ? ‚Äù asked Mrs . McKee , ‚Äú but full of cheerful sun . Sitting down behind the tall apartments of the sofa and his mistress , until , as we drove away . \\x03 \\x02 It was the man smoking two cigarettes . We ‚Äô ll want to see if we mirrored his unbelief . But it looks wonderful on you , see ? ‚Äù \\x03 \\x02 ‚Äú San Francisco . ‚Äù \\x03 \\x02 ‚Äú What if I had come East . ‚Äù \\x03 \\x02 I agreed that it wasn ‚Äô t been there before . I said to be taken out under any circumstances ‚Äî and loved me too ? ‚Äù \\x03 \\x02 Another pause . \\x03 \\x02 ‚Äú No , old sport ? ‚Äù \\x03 \\x02 ‚Äú At the enchanted metropolitan twilight I felt a sort of people come who haven ‚Äô t reach Gad ‚Äô s an Oxford man ! ‚Äù shouted Mrs . Wilson . Cars bought and sold grain alcohol over the most insidious flat on Lake Superior . It understood you just come ? I wouldn ‚Äô t come out frankly and tell me , for she turned a page with a tower on one Saturday night he added ‚Äú sir ‚Äù in a pyramid of pulpless halves . There was dancing now on the other . ‚Äù \\x03 \\x02 There was a permanent move , ‚Äù she cried and cried like a young cadet . Her face , tipped sideways beneath a three months ‚Äô trip to the Red Cross to make an accident . However glorious might be offended . You were his closest friend , without any particular wonder . \\x03 \\x02 We all looked ‚Äî and now I saw him opening a chest of rubies to ease , with shell - rimmed glasses and wiped them again , and as we strolled out among the blue cool limit of the gutter . I \\x03'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sample sentence built from probability table\n",
    "ngram_sample = ngram.sample(500) \n",
    "ngram_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f0d2e-c86b-4e61-8401-78b85db063a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
